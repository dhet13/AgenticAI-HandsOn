{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54de0ada",
   "metadata": {},
   "source": [
    "# Session 2: [Fast-Track] AI ë¶„ì„ ì „ë¬¸ê°€ êµ¬ì¶• ë° ë°°í¬ ì‹œì‘\n",
    "\n",
    "## ğŸ¯ ì„¸ì…˜ ëª©í‘œ\n",
    "ë³¸ ì„¸ì…˜ì€ **50ë¶„** ì•ˆì— ê³ ì„±ëŠ¥ **AI ë¶„ì„ ì „ë¬¸ê°€ (ëŒ€ì¶œ ì‹¬ì‚¬ ëª¨ë¸)**ë¥¼ êµ¬ì¶•í•˜ê³  ë°°í¬í•˜ëŠ” ëª¨ë“  ê³¼ì •ì„ ì••ì¶• ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "-   **XGBoost** ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "-   ëª¨ë¸ í•™ìŠµ í›„ GCSì— ì €ì¥í•˜ê³  Vertex AI Endpoint ë°°í¬ë¥¼ **ì‹œì‘**í•©ë‹ˆë‹¤.\n",
    "-   ë°°í¬ê°€ ì™„ë£Œë˜ê¸¸ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë°”ë¡œ ë‹¤ìŒ ì„¸ì…˜ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\n",
    "\n",
    "### âš ï¸ ì¤‘ìš” í™˜ê²½ ì„¤ì •\n",
    "Vertex AI ì»¨í…Œì´ë„ˆì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ **scikit-learn ë²„ì „ì„ ìë™ìœ¼ë¡œ 1.5.0ìœ¼ë¡œ ë§ì¶¥ë‹ˆë‹¤.** ì½”ë“œë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4773864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ì½”ë“œ ì…€ 1] í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sklearn\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier # ê³ ì„±ëŠ¥ ëª¨ë¸ì¸ XGBoost ì„í¬íŠ¸\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- (1) XGBoost ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---\n",
    "# Vertex AI Workbenchì—ëŠ” ê¸°ë³¸ìœ¼ë¡œ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª…ì‹œì ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "print(\"XGBoost ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "!pip install xgboost --quiet\n",
    "print(\"XGBoost ì„¤ì¹˜ ì™„ë£Œ.\")\n",
    "\n",
    "# --- (2) Scikit-learn ë²„ì „ ê²€ì‚¬ ë° ì¡°ì • ---\n",
    "REQUIRED_SKLEARN_VERSION = \"1.5.0\"\n",
    "\n",
    "if sklearn.__version__ != REQUIRED_SKLEARN_VERSION:\n",
    "    print(f\"ê²½ê³ : í˜„ì¬ scikit-learn ë²„ì „({sklearn.__version__})ì´ ìš”êµ¬ ë²„ì „({REQUIRED_SKLEARN_VERSION})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤.\")\n",
    "    print(f\"ìë™ìœ¼ë¡œ {REQUIRED_SKLEARN_VERSION} ë²„ì „ìœ¼ë¡œ ë‹¤ìš´ê·¸ë ˆì´ë“œ/ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    !pip install scikit-learn=={REQUIRED_SKLEARN_VERSION} --quiet\n",
    "    \n",
    "    # ì„¤ì¹˜ í›„ ì»¤ë„ ì¬ì‹œì‘ ì•ˆë‚´ (í•„ìˆ˜)\n",
    "    print(f\"\\n====================================================================================\")\n",
    "    print(f\"â–¶ scikit-learn {REQUIRED_SKLEARN_VERSION} ì„¤ì¹˜ ì™„ë£Œ! \")\n",
    "    print(f\"â–¶ **JupyterLab 'Kernel' -> 'Restart Kernel...'ì„ í´ë¦­í•˜ì—¬ ì»¤ë„ì„ ì¬ì‹œì‘**í•œ í›„,\")\n",
    "    print(f\"â–¶ ì´ ì…€ë¶€í„° ë‹¤ì‹œ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\")\n",
    "    print(f\"====================================================================================\")\n",
    "else:\n",
    "    print(f\"í˜„ì¬ scikit-learn ë²„ì „: {sklearn.__version__}. í™˜ê²½ ì„¤ì • ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28b732",
   "metadata": {},
   "source": [
    "## ğŸ“ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "* ML ì „ë¬¸ê°€ ëª¨ë¸ì€ ìˆ«ìë¡œ ëœ ë°ì´í„°ë§Œ ì´í•´\n",
    "* ì´ ë‹¨ê³„ì—ì„œëŠ” ë³µì¡í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ì •ì˜\n",
    "* ì´ í•¨ìˆ˜ëŠ” ì›ë³¸ ë°ì´í„°ë¥¼ ë°›ì•„ **ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³ , í…ìŠ¤íŠ¸(ë²”ì£¼í˜•) ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜(One-Hot Encoding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ì½”ë“œ ì…€ 2] ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ML ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. ë¶ˆí•„ìš”í•œ 'Loan_ID' ì»¬ëŸ¼ ì œê±°\n",
    "    df = df.drop('Loan_ID', axis=1, errors='ignore')\n",
    "    \n",
    "    # 2. ê²°ì¸¡ì¹˜(NaN) ì²˜ë¦¬: ìµœë¹ˆê°’ê³¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "    cat_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    num_cols = ['LoanAmount', 'Loan_Amount_Term']\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "    # 3. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (One-Hot Encoding)\n",
    "    df_processed = pd.get_dummies(df, drop_first=True, columns=[\n",
    "        'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'\n",
    "    ])\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (preprocess_data) ì •ì˜ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ì½”ë“œ ì…€ 3] ë°ì´í„° ë¡œë“œ, í•™ìŠµ ë° ì•„í‹°íŒ©íŠ¸ ì €ì¥\n",
    "\n",
    "# ğŸš¨ ì‚¬ìš©ì ì„¤ì •ê°’ (ë°˜ë“œì‹œ ìˆ˜ì •)\n",
    "BUCKET_NAME = \"ì—¬ëŸ¬ë¶„ì˜-GCS-ë²„í‚·-ì´ë¦„\" # ì˜ˆ: \"gcp-project-12345-loan-data\"\n",
    "TRAIN_FILE = \"train_u6lujuX_CVtuZ9i.csv\" # Kaggle ëŒ€ì¶œ ì‹¬ì‚¬ ì›ë³¸ íŒŒì¼ëª…\n",
    "\n",
    "# GCS ê²½ë¡œ ì„¤ì •\n",
    "GCS_PATH = f\"gs://{BUCKET_NAME}/{TRAIN_FILE}\"\n",
    "\n",
    "# GCSì—ì„œ ë°ì´í„° ë¡œë“œ (Step 1)\n",
    "try:\n",
    "    df = pd.read_csv(GCS_PATH)\n",
    "    print(f\"1. GCSì—ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ. ì´ {df.shape[0]} ê±´.\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ ì˜¤ë¥˜: GCS ë¡œë“œ ì‹¤íŒ¨! ë²„í‚· ì´ë¦„ ë˜ëŠ” íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}\")\n",
    "    sys.exit() # ì˜¤ë¥˜ ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì¤‘ë‹¨\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ (Step 2)\n",
    "df_processed = preprocess_data(df)\n",
    "df_processed['Loan_Status'] = df_processed['Loan_Status_Y'].astype(int)\n",
    "\n",
    "# í”¼ì²˜(X)ì™€ íƒ€ê²Ÿ(y) ë¶„ë¦¬ ë° í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬\n",
    "X = df_processed.drop('Loan_Status_Y', axis=1)\n",
    "y = df_processed['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"2. ë°ì´í„° ì „ì²˜ë¦¬ ë° í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "# XGBoost ëª¨ë¸ í•™ìŠµ (Step 3)\n",
    "model = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"3. XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ê²€ì¦ ì •í™•ë„: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "\n",
    "# ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë¡œì»¬ ì €ì¥ (Step 4)\n",
    "MODEL_DIR = 'models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "joblib.dump(model, os.path.join(MODEL_DIR, 'loan_approval_model.pkl'))\n",
    "joblib.dump(X.columns.tolist(), os.path.join(MODEL_DIR, 'model_features.pkl'))\n",
    "print(\"4. ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë¡œì»¬ ì €ì¥ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7a545",
   "metadata": {},
   "source": [
    "## ğŸ“ Vertex AI ë“±ë¡ ë° ë°°í¬ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ GCSì— ì—…ë¡œë“œí•˜ê³  Model Registryì— ë“±ë¡í•œ ë’¤, Endpointì— ë°°í¬í•˜ëŠ” ëª¨ë“  ê³¼ì •ì„ í•¨ìˆ˜ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "ì´ í•¨ìˆ˜ëŠ” ë³µì¡í•œ ë‹¨ê³„ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ì½”ë“œ ì…€ 4] Vertex AI ìì› ë“±ë¡ ë° ë°°í¬ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def register_and_deploy_model(project_id, region, bucket_name, endpoint_name, model_name):\n",
    "    \n",
    "    # 1. GCS ì—…ë¡œë“œ (Step 5)\n",
    "    MODEL_GCS_PATH = f\"gs://{bucket_name}/loan_model/v1/\"\n",
    "    ARTIFACT_GCS_PATH = f\"gs://{bucket_name}/loan_model/artifacts/\"\n",
    "\n",
    "    # ëª¨ë¸ íŒŒì¼ ë° í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ë¥¼ GCSì— ë¶„ë¦¬í•˜ì—¬ ì—…ë¡œë“œ (UI ë“±ë¡ ì˜¤ë¥˜ ë°©ì§€)\n",
    "    !gsutil cp models/loan_approval_model.pkl {MODEL_GCS_PATH}\n",
    "    !gsutil cp models/model_features.pkl {ARTIFACT_GCS_PATH}\n",
    "    print(\"1. GCS ì—…ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "    # 2. Model Registry ë“±ë¡ (Step 6)\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    # Model Registryì— ë“±ë¡\n",
    "    uploaded_model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        artifact_uri=MODEL_GCS_PATH, # GCS í´ë” ê²½ë¡œ\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-gpu.1-5:latest\", # XGBoost ì»¨í…Œì´ë„ˆ (ë²„ì „ì€ Scikit-learn ì»¨í…Œì´ë„ˆì™€ í˜¸í™˜ë˜ëŠ” ìµœì‹  ë²„ì „ ì‚¬ìš©)\n",
    "        serving_container_environment_variables={\"MODEL_FILENAME\": \"loan_approval_model.pkl\"},\n",
    "        sync=True # ë™ê¸°ì‹ ë“±ë¡\n",
    "    )\n",
    "    print(f\"2. Model Registry ë“±ë¡ ì™„ë£Œ. ëª¨ë¸ ID: {uploaded_model.name}\")\n",
    "    \n",
    "    # 3. Endpoint ë°°í¬ ì‹œì‘ (Step 7)\n",
    "    # Endpoint ë¦¬ì†ŒìŠ¤ ì •ì˜\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=endpoint_name, project=project_id, location=region)\n",
    "    \n",
    "    # ëª¨ë¸ ë°°í¬ ì‹œì‘ (ë¹„ë™ê¸° ì²˜ë¦¬)\n",
    "    endpoint.deploy(\n",
    "        model=uploaded_model,\n",
    "        deployed_model_display_name=f\"{model_name}-deployed\",\n",
    "        machine_type=\"n1-standard-2\", # ë°°í¬ VM ì‚¬ì–‘\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "        sync=False # ë¹„ë™ê¸° ë°°í¬ (í•¨ìˆ˜ë¥¼ ì¦‰ì‹œ ì¢…ë£Œí•˜ê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)\n",
    "    )\n",
    "    print(\"3. Endpoint ë°°í¬ ìš”ì²­ ì‹œì‘. ë°°í¬ ì™„ë£Œê¹Œì§€ 10~15ë¶„ ì†Œìš”ë©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    return endpoint\n",
    "\n",
    "print(\"Vertex AI ìì› ë“±ë¡ ë° ë°°í¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be26379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ì½”ë“œ ì…€ 5] ìµœì¢… ë“±ë¡ ë° ë°°í¬ ì‹¤í–‰\n",
    "\n",
    "# ğŸš¨ ì‚¬ìš©ì ì„¤ì •ê°’ (ì½”ë“œ ì…€ 1ì˜ ê°’ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)\n",
    "PROJECT_ID = \"ì—¬ëŸ¬ë¶„ì˜-GCP-PROJECT-ID\"\n",
    "BUCKET_NAME = \"ì—¬ëŸ¬ë¶„ì˜-GCS-ë²„í‚·-ì´ë¦„\"\n",
    "REGION = \"asia-northeast3\"\n",
    "\n",
    "# ë°°í¬í•  Endpointì™€ Model ì´ë¦„ ì •ì˜\n",
    "ENDPOINT_NAME = \"loan-approval-service\"\n",
    "MODEL_NAME = \"loan-analysis-expert\"\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰ (ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œì‘)\n",
    "deployed_endpoint = register_and_deploy_model(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "print(\"\\n====================================================================================\")\n",
    "print(\"â–¶â–¶â–¶ Session 2 ì™„ë£Œ. Endpoint ë°°í¬ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. â—€â—€â—€\")\n",
    "print(\"â–¶ ë‹¤ìŒ ì„¸ì…˜(Session 3)ì—ì„œ ì´ Endpointë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.\")\n",
    "print(\"====================================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-handson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
